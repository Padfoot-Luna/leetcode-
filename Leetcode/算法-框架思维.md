[TOC]
---
> :thinking: 递归和迭代是*实现手段*，分治策略、动态规划、贪心是解决问题的*思想*。

#### 递归

> 重复调用函数自身实现循环称为`递归`。
>
> 本质是能把问题拆分成具有**相同解决思路**的子问题， 经过层层分解的子问题最后一定是有一个不能再分解的固定值的（即**终止条件**）。
>
> 计算时间复杂度： `Master Theorm` 
>
> 空间复杂度：递归深度

#### 迭代

> `迭代`是函数内某段代码实现循环，使用**计数器**结束循环。
>
> 迭代与普通循环的区别是：循环代码中参与运算的变量同时是保存结果的变量，当前保存的结果作为下一次循环计算的初始值，利用变量的原值推出新值。



#### 所有的递归可以转换为迭代，但迭代不一定可以转换成递归

- 一部分`递归`算法，可以转成`动态规划`，实现空间换时间, 例如 [5题](https://leetcode.windliang.cc/leetCode-5-Longest-Palindromic-Substring.html)，[10题](https://leetcode.windliang.cc/leetCode-10-Regular-Expression-Matching.html)，[53题](https://leetcode.windliang.cc/leetCode-53-Maximum-Subarray.html?h=动态规划)，[72题](https://leetcode.wang/leetCode-72-Edit-Distance.html)，从自顶向下再向顶改为了自底向上。分析问题我们需要采用**自上而下**的思维，而解决问题有时候采用**自下而上**的方式能让算法性能得到极大提升。
- 一部分`递归`算法，只是可以用`栈`去模仿递归的过程，对于时间或空间的复杂度没有任何好处。

---

#### `分治 Divide and Conquer`

将原问题分解为若干个规模较小但类似于原问题的子问题（Divide），递归的求解这些子问题（Conquer），然后再合并这些子问题的解来建立原问题的解。一般用`递归`实现。



#### `动态规划 Dynamic Programming`

> 递归是从问题的结果倒推，直到问题的规模缩小到寻常。 动态规划是从寻常入手， 逐步扩大规模到**「最优子结构」**。

动态规划可以理解为是**查表**的递归, **用来解决存在「重叠子问题」的问题**。递归中可能存在很多的重复计算，一种简单的方式就是记忆化递归。即一边递归一边使用“记录表”记录我们已经计算过的情况，这样就避免了重复计算。而动态规划中 DP 数组其实和“记录表”一样。

动态规划问题的一般形式就是**求最值**，求解动态规划的核心问题是**穷举**。一般由两种方法来实现，一种为自顶向下的带备忘录的`递归`，一种为自底向上的`迭代`。

只有列出**正确的「状态转移方程」**才能正确地穷举。找**最优子结构**的过程，其实就是证明状态转移方程正确性的过程。

**明确 base case -> 明确「状态」-> 明确「选择」 -> 定义 dp 数组/函数的含义**。

```python
# 初始化 base case
dp[0][0][...] = base
# 进行状态转移
for 状态1 in 状态1的所有取值：
    for 状态2 in 状态2的所有取值：
        for ...
            dp[状态1][状态2][...] = 求最值(选择1，选择2...)
```

只要写出状态转移方程即暴力解法，优化方法无非是用备忘录或者 DP table，即「**状态压缩**」，再无奥妙可言。



#### `背包问题`
> 有N件物品和一个容量为V的背包。第i件物品的费用是c[i]，价值是w[i]。求解将哪些物品装入背包可使这些物品的费用总和不超过背包容量，且价值总和最大。

##### 1. 0-1 背包问题：每种物品数量为 1，放或不放

$f[i][v]$表示前 $i$ 件物品放入一个容量为$v$的背包可以获得的最大价值。

$f[i][v]=\max\{f[i-1][v], f[i-1][v-c[i]]+w[i]\}$    # 不放 or 放

`优化`：时间和空间复杂度均为$O(N*V)$，其中时间复杂度基本已经不能再优化了，但空间复杂度却可以优化到$O(V)$。

```python
for i in range(N):   # # 枚举前 i 个物品
	for v in range(V, c[i]-1, -1):  # 枚举体积，逆推f[v]保证推f[v]时f[v-c[i]]保存的是f[i-1][v-c[i]]的值
		f[v] = max(f[v], f[v-c[i]]+w[i])
```

---

##### 2. 完全背包问题：每种物品都有无限件可用

$f[i][v]=\max\{f[i-1][v-k*c[i]]+k*w[i]\;|\;0\leq k\leq v/c[i]\}$

```python
for i in range(N):  # 枚举前 i 个物品
    for v in range(c[i], V+1):  # 注意这里是正序遍历
        f[v] = max(f[v], f[v-c[i]]+w[i])
```

---

##### 3. 多重背包问题

##### 4. 混合背包问题

##### 5. 二维费用背包问题

##### 6. 分组背包问题

##### 7. 背包问题求方案数

##### 8. 背包问题的方案

##### 9. 有依赖的背包问题



#### `贪心算法 Greedy Algorithm`

贪心算法不从整体最优考虑，在每一步做出某种意义上的局部最优解，希望这样的选择能导致全局最优解。但只是寄希望，因此贪心算法并不保证得到最优解。既可以用`递归`实现，也可以用`迭代`实现。

关键是贪心策略的选择



#### `回溯 Backtracking`

回溯是一种通过穷举所有可能情况来找到所有解的算法。如果一个候选解最后被发现并不是可行解，回溯算法会舍弃它，并在前面的一些步骤做出一些修改，并重新尝试找到可行解。究其本质，回溯算法就是个 N 叉树的前后序遍历问题，没有例外。

```python
result = []
def backtrack(路径, 选择列表):
    if 满足结束条件:
        result.add(路径)
        return

    for 选择 in 选择列表:
        # 做选择
        将该选择从选择列表移除
        路径.add(选择)
        backtrack(路径, 选择列表)
        # 撤销选择
        路径.remove(选择)
        将该选择再加入选择列表
```

比如，DFS。

```python
class Sulution():
    res = []    # 定义全局变量保存最终结果
    state = []  # 定义状态变量保存当前状态
    p,q,r       # 定义条件变量（一般条件变量就是题目直接给的参数）
    def back(状态，条件1，条件2，……):
        if # 不满足合法条件（可以说是剪枝）
            return
        elif # 状态满足最终要求
            res.append(state)   # 加入结果
            return 
        # 主要递归过程，一般是带有 循环体 或者 条件体
        for # 满足执行条件
        if  # 满足执行条件
            back(状态，条件1，条件2，……)
    back(状态，条件1，条件2，……)
    return res
```



#### `二分查找`

细节是魔鬼 https://mp.weixin.qq.com/s?__biz=MzAxODQxMDM0Mw==&mid=2247484507&idx=1&sn=36b8808fb8fac0e1906493347d3c96e6&source=41#wechat_redirect

```python
def binarySearch(nums, target):
	left, right = 0, len(nums)-1
    while:  # 注意「搜索区间」和 while 的终止条件
        mid = (right + left) // 2;
        if nums[mid] == target:  # 如需要搜索左右边界，只要在 nums[mid] == target 
            ...   
        elif nums[mid] < target:
            left = ...
        else:  # nums[mid] > target
            right = ...
    return
```



#### `滑动窗口`

```python
/* 滑动窗口算法框架 */
def slidingWindow(string s, string t):
    unordered_map<char, int> need, window;
    for (char c : t) need[c]++;

    left = right = 0
    valid = 0
    while right < s.size():
        # c 是将移入窗口的字符
        c = s[right]
        right += 1  # 右移窗口
        # 进行窗口内数据的一系列更新
        ...

        # debug 输出的位置
        print("window: [%d, %d)\n", left, right)
        
        // 判断左侧窗口是否要收缩
        while window needs shrink:
            # d 是将移出窗口的字符
            d = s[left]
            left +=1  # 左移窗口
            # 进行窗口内数据的一系列更新
            ...
```



---

#### 时间复杂度

- `递归`算法的时间复杂度怎么计算？就是用子问题个数(即递归树中节点的总数)乘以解决一个子问题需要的时间。



#### 空间复杂度

- `递归`是将一个问题分解为若干相对小一点的问题，遇到递归出口再原路返回，因此必须保存相关的中间值，这些中间值压入栈保存，问题规模较大时会占用大量内存。

- `迭代`是逐渐逼近，用新值覆盖旧值，直到满足条件后结束，不保存中间值，空间利用率高。



---

####  Math

##### [XOR][]

恒等律：`X ⊕ 0 = X`      

归零律：`X ⊕ X = 0`

交换律：`A ⊕ B = B ⊕ A`

结合律：`A ⊕ (B ⊕ C) = (A ⊕ B) ⊕ C`

自反律：`A ⊕ B ⊕ B = A ⊕ 0 = A`    去重

##### [mod][]

结合律：`(a*b)%k = (a%k)(b%k)%k`   



---

## 图

### 1. 最短路径

#### 1.1单源最短路径 - Dijkstra: 

- BFS + Greedy
- 要求图中边的权值均为非负

```markdown
1. 初始化：声明一个数组dis来保存源点到各个顶点的最短距离和一个保存已经找到了最短路径的顶点的集合：T。初始时，原点 s 的路径权重被赋为 0 （dis[s] = 0）。若对于顶点 s 存在能直接到达的边（s,m），则把dis[m]设为w(s,t) ,同时把所有其他（s不能直接到达的）顶点的路径长度设为无穷大。初始时，集合T只有顶点s。
2. 从dis数组选择最小值，则该值就是源点s到该值对应的顶点的最短路径，并且把该点加入到T中，OK，此时完成一个顶点，
3. 我们需要看看新加入的顶点是否可以到达其他顶点并且看看通过该顶点到达其他点的路径长度是否比源点直接到达短，如果是，那么就替换这些顶点在dis中的值。
4. 重复2和3，直到T中包含了图的所有顶点。
```

- 时间复杂度：$O(V^2)$。 dis用heap，复杂度可以降为$O(E+V\log V)$
- 空间复杂度：$O(V)$

---

#### 1.2 单源最短路径 - SPFA  / aka Moore-Bellman-Ford

- Dynamic programming
- 权值可为负

```markdown
1. 用数组dis记录每个结点的最短路径估计值，用邻接表或邻接矩阵来存储图G
2. 采取动态逼近法：设立一个先进先出的队列用来保存待优化的结点，优化时每次取出队首结点u，并且用u点当前的最短路径估计值对u点所指向的结点v进行**松弛操作**，如果源点到v点的最短路径有所调整，且v点不在当前的队列中，就将v点放入队尾。这样不断从队列中取出结点来进行松弛操作，直至队列空为止
3. 带有负环的图是没有最短路径的，所以我们在执行算法的时候，要判断图是否带有负环，方法有两种：
	    1. 开始算法前，调用拓扑排序进行判断（一般不采用，浪费时间）
        2. 如果某个点进入队列的次数超过N次则存在负环（N为图的顶点数）
```

- 时间复杂度：$O(EV)$。
- 空间复杂度：$O(V)$

---

#### 1.3 多源最短路径 - Floyd

```markdown
1. 初始化：矩阵D中的元素a[i][j]表示顶点i到顶点j的距离。
        (如果需要最短路径）矩阵P中的元素b[i][j]，表示顶点i到顶点j经过了顶点b[i][j]
2. 对矩阵D进行N次更新。
        第k次更新时，如果a[i][j]>a[i][k-1]+a[k-1][j]，则更新a[i][j]=a[i][k-1]+a[k-1][j],b[i][j]=b[i][k-1]
```

- 时间复杂度：$O(V^3)$。

- 空间复杂度：$O(V^2)$

---

### 2. 动态连通性 - Union-Find 并查集

主要是解决图论中「动态连通性」问题

并查集由一个整数型的数组和两个函数构成。数组parent[]记录了每个点的前导点是什么，函数find是查找根结点，union是合并两个结点。

```python
parent = {i:i for i in nums}
size = {i:1 for i in nums}
count = n
for i in range(n):
    parent[i] = i
    size[i] = 1
    
def find(x):
    while x != parent[x]:
        parent[x] = parent[parent[x]]  # 路径压缩：所有树高都不会超过 3
        x = parent[x]
    return x

def union(x, y):
    p, q = find(x), find(y)
    if p != q:
        count -= 1
        if size[p] < size[q]:  # 小树接到大树下面，较平衡
        	parent[p] = q
            size[q] += size[p]
        else:
            parent[q] = p
            size[p] += size[q]
```

### 3. Topological Sort

```

```








---
> - $n$：数据规模
> - $k$：“桶”的个数
> - **In-place**：占用常数内存，不占用额外内存
> - **Out-place**：占用额外内存
> - **稳定性**：排序后2个相等键值的顺序和排序之前它们的顺序相同
## 内部排序

### 插入排序 

#### 1. 直接插入排序 Insertion sort

> :bulb:：每次将一个数字插入一个有序的数组使之成为一个长度更长的有序数组，有限次操作以后，数组整体有序。
>
> 特点：插入排序可以提前终止内层循环，在数组几乎有序的前提下，「插入排序」的时间复杂度可以达到 $O(n)$
>
> [稳定]()

```python
def insertionSort(nums):
    for i in range(len(nums)):
        j = i  # 将nums[i]插入有序数组nums[:i]
        while j>0 and nums[j-1] > nums[j]:
            nums[j-1], nums[j] = nums[j], nums[j-1] 
            j -= 1
    return nums 
```

- 时间复杂度：$O(n^2)$，最好情况$O(n)$。$\sum_{i=1}^n i$
- 空间复杂度：$O(1)$。

---

#### 2. 希尔排序 Shell sort

> :bulb:：插入排序的优化：在插入排序里，如果靠后的数字较小，它来到前面就得交换多次。「希尔排序」改进了这种做法。**带间隔**地使用插入排序，直到最后「间隔」为 1 就是标准的「插入排序」，此时数组里的元素已经「几乎有序」了。
>
> 希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。

```python
def shellSort(nums):
    n = len(nums)
    gap = n//2
    while gap:   # log n
        for i in range(n):   # n
            j = i
            while j-gap>=0 and nums[j-gap] > nums[j]:   # 
                nums[j-gap], nums[j] = nums[j], nums[j-gap]    # 不稳定: 子序列内部进行插入排序
                j -= gap
        gap //= 2
    return nums
```

- 时间复杂度：$O(n\log^2 n)$，最坏情况$O(n^2)$。
- 空间复杂度：$O(1)$。

---

### 选择排序

#### 3. 简单选择排序 Selection sort

> :bulb:：每一轮选取未排定的部分中**最小**的部分**交换**到未排定部分的最开头，经过若干个步骤，就能排定整个数组。即：先选出最小的，再选出第 2 小的，以此类推。
>
> 算法思想 1：贪心算法：每一次决策只看当前，当前最优，则全局最优。注意：这种思想不是任何时候都适用。
> 算法思想 2：减治思想：外层循环每一次都能排定一个元素，问题的规模逐渐减少，直到全部解决，即「大而化小，小而化了」。运用「减治思想」很典型的算法就是大名鼎鼎的「二分查找」。
> 优点：交换次数最少。
> [不稳定](https://blog.csdn.net/houyanjun/article/details/2446074): 通过**交换**未排定部分的头值与最小值来达到将最小值放到开头的目的，所以不稳定。

```python
def selectionSort(nums):
    for i in range(len(nums)):
        tmp = i   # 找到nums[i:]中最小值的位置，与i上的值交换
        for j in range(i+1, len(nums)):
            if nums[j] < nums[tmp]:
                tmp = j
        nums[i], nums[tmp] = nums[tmp], nums[i]  # 不稳定
    return nums
```

- 时间复杂度：$O(n^2)$  $\sum_{i=n}^1 i$ 
- 空间复杂度：$O(1)$

---

#### 4. 堆排序 Heap sort :star:

> :bulb:：选择排序的优化：选择排序需要在未排定的部分里通过「打擂台」的方式选出最大的元素（复杂度 $O(n)$），而「堆排序」就把未排定的部分构建成一个「堆」，这样就能以 $O(\log n)$的方式选出最大元素。
>
> 大根堆：每个节点的值都大于或等于其子节点的值，用于升序排列；
> 小根堆：每个节点的值都小于或等于其子节点的值，用于降序排列。
>
> 堆是一种相当有意思的数据结构，它在很多语言里也被命名为「优先队列」。它是**建立在数组上的「树」结构**，类似的数据结构还有「并查集」「线段树」等

```python
def heapSort(nums):   # 大根堆（从小打大排列）
    def adjustHeap(nums, i, n):  # 调整堆， 递归深度log n 
        # 非叶子结点 i 的左右两个孩子
        lchild = 2 * i + 1
        rchild = 2 * i + 2
        # 在当前结点、左孩子、右孩子中找到最大元素的索引
        largest = i 
        if lchild < n and nums[lchild] > nums[largest]: 
            largest = lchild 
        if rchild < n and nums[rchild] > nums[largest]: 
            largest = rchild 
        # 如果最大元素的索引不是当前结点，把大的结点交换到上面，继续调整堆
        if largest != i: 
            nums[largest], nums[i] = nums[i], nums[largest] 
            # largest 的索引是交换前大数字对应的索引, 交换后该索引对应的是小数字，应该把该小数字再向下调整
            adjustHeap(nums, largest, n)
    
    def builtHeap(nums, n):  # 建立堆
        for i in range(n//2-1,-1,-1): # 从倒数第一个非叶子结点开始建立大根堆，即无叶子结点：2*i+1<n
            adjustHeap(nums, i, n) # 对所有非叶子结点进行堆的调整
   
    n = len(nums)
    builtHeap(nums, n) 
    for i in range(n-1,-1,-1):  
        # 每次根结点nums[0]都是未排序部分最大的数，最大数换到未排序的末位，即nums[i]
        nums[0], nums[i] = nums[i], nums[0]    # ！！！不稳定
        # 交换完后还需要继续调整堆，只需调整根节点，此时数组是未排序部分nums[:i]
        adjustHeap(nums, 0, i) 
    return nums  # 由于每次大的都会放到后面，因此最后的 nums 是从小到大排列
```

- 时间复杂度：$O(n\log n)$， 初始化建堆的时间复杂度为 $O(n)$，建完堆以后需要进行 $n-1$ 次调整，一次调整的时间复杂度为 $O(\log n)$。 

    `建堆的时间复杂度分析：`

    - $k=\log n$ 表示堆的高度
    - $i$  表示所在层，该层有 $2^{i-1}$ 个结点， 对该层非叶子结点进行堆调整，递归深度为$k-i$
    - $s = \sum_{i=k-1}^{i=1} 2^{i-1} * (k-i)$： 从倒数第二层自下而上到第一层建堆
    - $s=2^0*(k-1) + 2^1*(k-2) + \cdots + 2^{k-2}*1$
    - $2s=2^1*(k-1) + 2^2*{k-2} + \cdots + 2^{k-1}*1$
    - $s=2s-s=2^{k-1}-(k-1) + \sum_{i=1}^{i=k-2}2^i=2^{k-1}-(k-1)+2^{k-1}-2=2^k-k-1=O(n)$ 

- 空间复杂度：$O(1)$，递归无回溯。

---

### 交换排序

#### 5. 冒泡排序 Bubble sort

> :bulb:：外层循环每一次经过两两比较，把每一轮**未排定部分最大的元素放到了数组的末尾**；
> 特点：在遍历的过程中，提前检测到数组是有序的，从而结束排序，而不像「选择排序」那样，即使输入数据是有序的，「选择排序」依然需要「傻乎乎」地走完所有的流程。
>
> [稳定]()

```python
def bubbleSort(nums):
    n = len(nums)
    count = n
    while count:
        count = 0
        for i in range(1,n):
            if nums[i-1] > nums[i]:
                nums[i-1], nums[i] = nums[i], nums[i-1]
                count += 1
    return nums
```

- 时间复杂度：$O(n^2)$，最好情况$O(n)$。
- 空间复杂度：$O(1)$。

---

#### 6. 快速排序 Quick sort :star:

> :bulb:：快速排序每一次都排定一个元素（这个元素呆在了它最终应该呆的位置），然后递归地去排它左边的部分(都比当前元素值小）和右边的部分（都比当前元素大），依次进行下去，直到数组有序。
>
> 算法思想：分而治之（分治思想），与「归并排序」不同，「快速排序」在「分」这件事情上不像「归并排序」无脑地一分为二，而是采用了 partition 的方法，因此就没有「合」的过程。
>
> [不稳定](): 非常脆弱，在实现时要非常小心才能避免低劣的性能。

```python
def quickSort(nums):  # 这种写法的平均空间复杂度为 O(nlogn)
    if len(nums) <= 1:
        return nums
    pivot = nums[0]  # 基准值
    left = [x for x in nums[1:] if x < pivot] 
    right = [x for x in nums[1:] if x >= pivot]
    return quickSort(left) + [pivot] + quickSort(right)

def quickSort2(nums, left, right):  # 这种写法的平均空间复杂度为 O(logn): 递归深度
    # 分区操作
    def partition(nums, left, right):
        pivotIndex = random.randint(left, right)
        nums[left], nums[pivotIndex] = nums[pivotIndex], nums[left]     
        pivot = nums[left]  # 基准值
        while left < right:
            # 如果 pivot=nums[left]，则先比较覆盖nums[left] = nums[right]，
            # 反之 pivot=nums[right]，先覆盖nums[right] = nums[left]
            while left < right and nums[right] >= pivot:
                right -= 1
            nums[left] = nums[right]  # nums[right]<pivot and left<=right 比基准小的交换到前面
            while left < right and nums[left] <= pivot:
                left += 1
            nums[right] = nums[left]  # nums[left]>pivot and left<=right  比基准大交换到后面
        nums[left] = pivot # 基准值的正确位置  left = right
        return left  # 返回基准值的索引
    # 递归操作
    if left < right:
        pivotIndex = partition(nums, left, right)
        quickSort2(nums, left, pivotIndex - 1)  # 左序列
        quickSort2(nums, pivotIndex + 1, right) # 右序列
    return nums
```

- 时间复杂度：$O(n\log n)$，最好情况$O(n\log n)$，最差$(n^2)$。

    它是处理**大数据**最快的排序算法之一，虽然 Worst Case 的时间复杂度达到了 $O(n^2)$，但是在大多数情况下都比平均时间复杂度为 $O(n\log n)$ 的排序算法表现要更好，因为 $O(n\log n)$  记号中隐含的**常数因子很小**，而且快速排序的内循环比大多数排序算法都要短小，这意味着它无论是在理论上还是在实际中都要更快，比复杂度恒定等于 $O(n\log n)$ 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。

- 空间复杂度：最好递归树相对平衡 $O(\log n)$，最差递归树倾斜$ O(n)$，递归函数的栈空间。

---

### 其余

#### 7. 归并排序 Merge sort

> :bulb:：借助额外空间，合并两个有序数组，得到更长的有序数组。
> 算法思想：分而治之（分治思想）。自底向上（递归、迭代）
>
> [稳定]()

```python
def mergeSort(nums):
    def merge(left, right):  # merge two ordered lists
        tmp = []
        i, j = 0, 0
        while i<len(left) and j<len(right):
            if left[i] <= right[j]:  # !!! ensure the stablility
                tmp.append(left[i])
                i += 1
            else:
                tmp.append(right[j])
                j += 1
        tmp = tmp + left[i:] + right[j:]
        return tmp
        
    if len(nums)<2: return nums
    mid = len(nums)//2
    left = mergeSort(nums[:mid])  # sort the left part
    right = mergeSort(nums[mid:]) # sort the right part
    return merge(left, right)
```

- 时间复杂度：$O(n\log n)$, 代价是需要额外的内存空间。
- 空间复杂度：$O(n)$

---
## 外部排序

#### 8. 计数排序 Counting sort

> :bulb:：要求输入数据的范围在 [0,N-1] 之间，则可以开辟一个大小为 N 的数组空间，将输入的数据值转化为键存储在该数组空间中，数组中的元素为该元素出现的个数。它是一种线性时间复杂度的排序。
>
> [稳定]()

```python
def countingSort(nums):
    bucket = [0] * (1+max(nums))
    for num in nums:
        bucket[num] += 1
    i = 0
    for num, count in enumerate(bucket):
        nums[i:i+count] = [num]*count
        i += count
    return nums
```
- 时间复杂度：$O(n+k)$
- 空间复杂度：$O(k)$
---

#### 9. 桶排序 Bucket sort :star:

> :bulb:：计数排序的升级版：高效与否的关键就在于这个映射函数的确定。同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。
>
> [稳定]()

```python
def bucketSort(nums, bucketSize = 5):  # 如果没有指定桶的大小，则默认为5
    minVal, maxVal = min(nums), max(nums)
    bucketCount = (maxVal - minVal) // bucketSize + 1  # 数据分为 bucketCount 组
    buckets = [[] for _ in range(bucketCount)] # 二维桶
    # 利用函数映射将各个数据放入对应的桶中
    for num in nums:
        buckets[(num - minVal)//bucketSize].append(num)
    nums.clear()  # 清空 nums
    # 对每一个二维桶中的元素进行排序
    for bucket in buckets:
        insertionSort(bucket)  # 假设使用插入排序
        nums.extend(bucket)    # 将排序好的桶依次放入到 nums 中
    return nums
```
- 时间复杂度：$O(n+k)$
- 空间复杂度：$O(n+k)$
---
#### 10. 基数排序 Radix sort

> :bulb:：桶排序的一种推广
>
> 1. MSD （主位优先法）：从高位开始进行排序
> 2. LSD （次位优先法）：从低位开始进行排序
>
> [稳定]()
```python
def radixSort(nums):  # LSD Radix Sort: 只给正数排序！！！
    mod = 10
    div = 1
    mostBit = len(str(max(nums)))  # 最大数的位数决定了外循环多少次
    buckets = [[] for _ in range(mod)] # 构造 mod 个空桶
    while mostBit:
        for num in nums:  # 将数据放入对应的桶中
            buckets[num // div % mod].append(num)
        nums = reduce(lambda x,y: x+y, buckets) # 将数据收集起来
        div *= 10
        mostBit -= 1
    return nums
```
- 时间复杂度：$O(k*n)$
- 空间复杂度：$O(k+n)$

Reference:

https://www.jianshu.com/p/bbbab7fa77a2

https://leetcode-cn.com/problems/sort-an-array/solution/fu-xi-ji-chu-pai-xu-suan-fa-java-by-liweiwei1419/







# 大数据算法

> 数据量很大可能是TB级别甚至是PB级别，导致无法一次性载入内存或者无法在较短时间内处理完成。面对海量数据，我们想到的最简单方法即是**分治法**，即分开处理，大而化小，小而治之。我们也可以想到集群分布式处理。

1B (Byte 字节)=8bit
1KB (Kilobyte 千字节) = 1024Bs
1MB (Megabyte 兆字节 简称“兆”) = 1024KB  (1024=2^10)
1GB (Gigabyte 吉字节 又称“千兆”) = 1024MB
1TB (Trillionbyte 万亿字节 太字节) = 1024GB
1PB (Petabyte 千万亿字节 拍字节) = 1024TB
1EB (Exabyte 百亿亿字节 艾字节) =1024PB
1ZB (Zettabyte 十万亿亿字节 泽字节) = 1024 EB
1YB (Yottabyte 一亿亿亿字节 尧字节) = 1024 ZB
1BB (Brontobyte 一千亿亿亿字节) = 1024 YB

## 1. Bloom Filter **布隆过滤器**

> **Bloom Filter（BF）是一个判断元素是否存在集合的快速的概率算法，有可能会出现错误判断，但不会漏掉判断。也就是Bloom Filter判断元素不在集合，那肯定不在。如果判断元素存在集合中，有一定的概率判断错误。**它是一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并能判断一个元素是否属于这个集合。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter相比其他常见的算法（如hash，折半查找），极大的节省了空间。
>
> - 优点是空间效率和查询时间都优于一般的算法：**add和query的时间复杂度都为O(k)，与集合中元素的多少无关。**
>
> - 缺点是有一定的误识别率和删除困难。

Bloom-Filter算法的核心思想就是利用多个独立的Hash函数来解决“冲突”。

Hash函数将一个元素（比如URL）映射到二进制位数组（位图数组）中的某一位，如果该位已经被置为1，只能说明该元素可能已经存在。因为Hash存在一个冲突（碰撞）的问题，即不同URL的Hash值有可能相同。为了减少冲突，我们可以多引入几个独立的hash函数，如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能在很大概率上认为该元素存在于集合中。

1. （不同）元素总数为n
2. m位的bit数组：每一个bit位都初始化为0。 $m=-\frac{n\cdot \ln p}{(\ln 2)^2}$
3. k个独立均匀分布的hash函数：为了添加一个元素，用k个hash函数将它hash得到bloom filter中k个bit位，将这k个bit位都置1(超过m的取余%m)。$k=\ln 2\cdot\frac{m}{n}=0.7\cdot\frac{m}{n}$
4. 误判概率p：为了查询一个元素是否在集合中，用k个hash函数将它hash得到k个bit位。若这k bits全为1，则此元素以概率**(1-p)**在集合中；若其中任一位不为1，则此元素必不在集合中（因为如果在，则在添加时已经把对应的k个bits位置为1）。$p=(1-e^{-\frac{nk}{m}})^k$

不允许移除元素，因为那样的话会把相应的k个bits位全置为0，而其中很有可能有其他元素对应的位。因此remove会引起误报，这是绝对不被允许的。而删除元素其实可以通过引入**白名单**解决。



## 2. Hash 散列表







## 3. Heap 堆





## 4. Bit-Map

> 对于排序，
>
> - 优点是：运算效率高，不需进行比较和移位，时间复杂度是O(n)；占用内存少，比如N=10000000；只需占用内存为N/8=1250000Byte=1.25M。
> - 缺点：数据最好是惆集数据(不然空间浪费很大)；数据不可重复(会将重复的数据覆盖掉)